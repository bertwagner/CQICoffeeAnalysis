{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37232bitvenvvirtualenv21d882e9f719476aaee9957a5bb220cc",
   "display_name": "Python 3.7.2 32-bit ('.venv': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Much of this code inspired by https://github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb from the book \"Hands-On Machine Learning with Scikit-Learn & TensorFlow\" by Aurélien Geron."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Setup defaults and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_data = pd.read_csv(\"datasets//arabica_data_cleaned.csv\", index_col=[0])\n",
    "coffee_data = coffee_data.dropna(subset=[\"Country.of.Origin\",\"Variety\",\"Processing.Method\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create test and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set =  train_test_split(coffee_data, test_size=0.2, random_state=17)\n",
    "\n",
    "coffee = train_set.drop(\"Total.Cup.Points\",axis=1)\n",
    "coffee_labels = train_set[\"Total.Cup.Points\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Write all of our data transformation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def null_altitude_outliers(X):\n",
    "    X[X < 200] = None\n",
    "    X[X > 5000] = None\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# We need these because if a value doesn't appear in the training set, it will fail in the test set\n",
    "unique_countries = list(coffee_data[\"Country.of.Origin\"].unique())\n",
    "unique_varieties = list(coffee_data[\"Variety\"].unique())\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "        ('country_encoding', Pipeline([\n",
    "            ('encoder', OrdinalEncoder(categories=[unique_countries]))\n",
    "        ]),[\"Country.of.Origin\"]),\n",
    "        ('variety_encoding', Pipeline([\n",
    "            ('encoder', OrdinalEncoder(categories=[unique_varieties]))\n",
    "        ]),[\"Variety\"]),\n",
    "        ('processing_encoding', Pipeline([\n",
    "            ('encoder', OrdinalEncoder())\n",
    "        ]),[\"Processing.Method\"]),\n",
    "        ('numerical_data', Pipeline([\n",
    "            ('null_outliers', FunctionTransformer(null_altitude_outliers,validate=False)),\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', MinMaxScaler())\n",
    "        ]),[\"altitude_mean_meters\"])\n",
    "    ])\n",
    "\n",
    "coffee_prepared = pipeline.fit_transform(coffee)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train on the prepared data.  Test various algorithms and their errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.609355162708351"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(coffee_prepared, coffee_labels)\n",
    "\n",
    "coffee_predictions = linear_regression.predict(coffee_prepared)\n",
    "linear_mse = mean_squared_error(coffee_labels,coffee_predictions)\n",
    "linear_rmse = np.sqrt(linear_mse)\n",
    "linear_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.547140261435914"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(coffee_prepared, coffee_labels)\n",
    "\n",
    "coffee_predictions = tree_reg.predict(coffee_prepared)\n",
    "tree_mse = mean_squared_error(coffee_labels,coffee_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.6830327124524418"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(coffee_prepared, coffee_labels)\n",
    "\n",
    "coffee_predictions = forest_reg.predict(coffee_prepared)\n",
    "forest_mse = mean_squared_error(coffee_labels,coffee_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Estimator Name: linear\nScores: [3.02849249 2.96519542 2.25563438 2.25502797 2.56039309 2.10010873\n 3.42839173 2.74842951 2.34846564 2.23623238]\nMean: 2.59263713373608\nStandard deviation: 0.4134423003353246\n\nEstimator Name: tree\nScores: [3.77521803 2.82958078 2.37696262 2.4319268  3.68829751 2.58867205\n 3.66951236 2.6078661  2.44266597 2.52819137]\nMean: 2.893889359118404\nStandard deviation: 0.5483136241345696\n\nEstimator Name: forest\nScores: [3.20400837 2.76156558 2.08012415 2.07747684 2.78563371 2.41456572\n 3.30699095 2.56735791 2.33476961 2.23463517]\nMean: 2.5767128019751118\nStandard deviation: 0.41260078019548674\n\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(name,scores):\n",
    "    print(\"Estimator Name:\", name)\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    print(\"\")\n",
    "\n",
    "scores = cross_val_score(linear_regression, coffee_prepared, coffee_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "linear_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(\"linear\",linear_rmse_scores)\n",
    "\n",
    "scores = cross_val_score(tree_reg, coffee_prepared, coffee_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(\"tree\",tree_rmse_scores)\n",
    "\n",
    "scores = cross_val_score(forest_reg, coffee_prepared, coffee_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(\"forest\",forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perform a grid search for hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'max_features': 1, 'n_estimators': 250}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [100,150,180,200,250,300,350,400], 'max_features': [1,2,3,4]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [100,150,180,200,250,300,350,400], 'max_features': [1,2,3,4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=17)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(coffee_prepared, coffee_labels)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.25412312, 0.189736  , 0.05873432, 0.49740656])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "#country,variety,processing,altitude\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The above feature importance indicates that altitude has the largest importance, followed by country of origin, variety, and finally processing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.6202736927843424"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = test_set.drop(\"Total.Cup.Points\",axis=1)\n",
    "y_test = test_set[\"Total.Cup.Points\"].copy()\n",
    "\n",
    "X_test_prepared = pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[84.23760324]\n"
    }
   ],
   "source": [
    "estimator = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
    "                      max_depth=None, max_features=4, max_leaf_nodes=None,\n",
    "                      max_samples=None, min_impurity_decrease=0.0,\n",
    "                      min_impurity_split=None, min_samples_leaf=1,\n",
    "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                      n_estimators=250, n_jobs=None, oob_score=False,\n",
    "                      random_state=17, verbose=0, warm_start=False)\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", pipeline),\n",
    "        (\"estimator\", estimator)\n",
    "    ])\n",
    "\n",
    "full_pipeline_with_predictor.fit(coffee, coffee_labels)\n",
    "\n",
    "data = coffee.copy().iloc[:1]\n",
    "data[\"Country.of.Origin\"] = \"Kenya\"\n",
    "data[\"Variety\"] = \"Bourbon\"\n",
    "data[\"Processing.Method\"] = \"Natural / Dry\"\n",
    "data[\"altitude_mean_meters\"] = 2000\n",
    "\n",
    "result = full_pipeline_with_predictor.predict(data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Save the model to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['coffee_model.pkl']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(full_pipeline_with_predictor, \"coffee_model.pkl\")"
   ]
  }
 ]
}